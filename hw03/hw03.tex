\documentclass[11pt]{article}

% ------
% LAYOUT
% ------
\textwidth 165mm %
\textheight 230mm %
\oddsidemargin 0mm %
\evensidemargin 0mm %
\topmargin -15mm %
\parindent= 10mm

\usepackage[dvips]{graphicx}
\usepackage{multirow,multicol}
\usepackage[table]{xcolor}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsmath}

\usepackage{subfigure}
\usepackage{minted}

\graphicspath{{./pix/}} % put all your figures here.

\begin{document}
\begin{center}
\Large{\textbf{ECE 595: Homework 3}}

Yi Qiao, Class ID 187

(Spring 2019)
\end{center}

\section*{Exercise 1}
\subsection*{(a)}
\subsubsection*{(i)}
\noindent \textbf{To Prove:}$$\pmb{x}^T\pmb{Ax}=tr\left[\pmb{Axx}^T\right]$$

\begin{equation}
\begin{split}
\pmb{x}^T\pmb{Ax} &=\begin{bmatrix}
x_0 & x_1 & ... & x_{n-1}
\end{bmatrix}\begin{bmatrix}
a_{0,0} & a_{0,1} & ... & a_{0,n-1}\\
\vdots  &	\vdots & 	&	\vdots \\
a_{n-1,0}&	a_{n-1,1} & ... & a_{n-1, n-1}\\
\end{bmatrix}\begin{bmatrix}
x_0\\
x_1\\
\vdots\\
x_{n-1}\\
\end{bmatrix}\\
&=\sum_{j=0}^{n-1}\sum_{i=0}^{n-1}x_ia_{i,j}x_j\\
&=\sum_{i=0}^{n-1}\left(\sum_{j=0}^{n-1}a_{i,j}x_j\right)x_i\\
\end{split}
\end{equation}
\begin{equation}
\begin{split}
tr\left[\pmb{Axx}^T\right]&=tr\left[\begin{bmatrix}
-\ a_0\ -\pmb{x} \\
-\ a_1\ -\pmb{x} \\
\vdots \\
-\ a_{n-1}\ -\pmb{x}
\end{bmatrix}\pmb{x}^T\right]\\
&=tr\left[\begin{bmatrix}
-\ a_0\ -\pmb{x}x_{0} & -\ a_0\ -\pmb{x}x_1 & ... & -\ a_0\ -\pmb{x}x_{n-1} \\
-\ a_1\ -\pmb{x}x_{0} & -\ a_1\ -\pmb{x}x_1 & ... & -\ a_1\ -\pmb{x}x_{n-1}\\
\vdots & \vdots & ... & \vdots \\
-\ a_{n-1}\ -\pmb{x}x_{0} & -\ a_{n-1}\ -\pmb{x}x_{0} & ... & -\ a_{n-1}\ -\pmb{x}x_{0}
\end{bmatrix}\right]\\
&=\sum_{i=0}^{n-1}-\ a_i\ -\pmb{x}x_i\\
&=\sum_{i=0}^{n-1}\left(\sum_{j=0}^{n-1}a_{i,j}x_j\right)x_i
\end{split}
\end{equation}\\
Thus, the above two are equivalent.
\pagebreak
\subsubsection*{(ii)}

since the likelihood function is, $$p(\pmb{x}|\pmb{\Sigma})=\frac{1}{(2\pi)^{d/2}|\pmb{\Sigma}|^{1/2}}exp\left\{-\frac{1}{2}(\pmb{x}-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}-\pmb{\mu})\right\}$$

\begin{equation}
\begin{split}
p(\pmb{x}_1,...,\pmb{x}_N|\pmb{\Sigma})&=\prod_{i=1}^{N}\frac{1}{(2\pi)^{d/2}|\pmb{\Sigma}|^{1/2}}exp\left\{-\frac{1}{2}(\pmb{x}_i-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}_i-\pmb{\mu})\right\}\\
&=\frac{1}{(2\pi)^{Nd/2}}|\pmb{\Sigma}^{-1}|^{N/2}exp\left\{-\frac{1}{2}\sum_{i=1}^{N}(\pmb{x}_i-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}_i-\pmb{\mu})\right\}\\
\end{split}
\end{equation}\\

Using the property we found above, we get,

\begin{equation}
\begin{split}
p(\pmb{x}_1,...,\pmb{x}_N|\pmb{\Sigma})&=\frac{1}{(2\pi)^{Nd/2}}|\pmb{\Sigma}^{-1}|^{N/2}exp\left\{-\frac{1}{2}\sum_{i=1}^{N}tr\left[\pmb{\Sigma}^{-1}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]\right\}\\
&=\frac{1}{(2\pi)^{Nd/2}}|\pmb{\Sigma}^{-1}|^{N/2}exp\left\{-\frac{1}{2}tr\left[\sum_{i=1}^{N}\pmb{\Sigma}^{-1}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]\right\}\\
&=\frac{1}{(2\pi)^{Nd/2}}|\pmb{\Sigma}^{-1}|^{N/2}exp\left\{-\frac{1}{2}tr\left[\pmb{\Sigma}^{-1}\sum_{i=1}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]\right\}
\end{split}
\end{equation}

\subsubsection*{(iii)}
since 
$$\hat{\pmb{\Sigma}}_{MLE} = \frac{1}{N}\sum_{n=1}^{N}(\pmb{x}_n-\hat{\pmb{\mu}})(\pmb{x}_n-\hat{\pmb{\mu}})^T$$
we can rewrite the equation above as\\
\begin{equation}
\begin{split}
p(\pmb{x}_1,...,\pmb{x}_N|\pmb{\Sigma})&=\frac{1}{(2\pi)^{Nd/2}}|\pmb{A}\hat{\pmb{\Sigma}}_{MLE}^{-1}|^{N/2}exp\left\{-\frac{1}{2}tr\left[\pmb{\Sigma}^{-1}N\hat{\pmb{\Sigma}}_{MLE}\right]\right\}\\
&=\frac{1}{(2\pi)^{Nd/2}|\hat{\pmb{\Sigma}}_{MLE}|^{N/2}}|\pmb{A}|^{N/2}exp\left\{-\frac{N}{2}tr\left[A\right]\right\}\\
\end{split}
\end{equation}
since $\pmb{A}$ is diagonalizable, $|\pmb{A}| = \prod_{i=1}^{d}\lambda_i$, plug it in, we get

\begin{equation}
\begin{split}
p(\pmb{x}_1,...,\pmb{x}_N|\pmb{\Sigma})&=\frac{1}{(2\pi)^{Nd/2}|\hat{\pmb{\Sigma}}_{MLE}|^{N/2}}(\prod_{i=1}^{d}\lambda_i)^{N/2}exp\left\{-\frac{N}{2}\sum_{i=1}^{d}\lambda_i\right\}\\
\end{split}
\end{equation}

\pagebreak
\subsubsection*{(iv)}
To maximize the likelihood function $$\underset{\lambda_1...\lambda_d}{argmax}\ p(x_1,...,x_N|\pmb{\Sigma})$$
we have,
\begin{equation}
\begin{split}
\pmb{\lambda}&=\underset{\lambda_1...\lambda_d}{argmax}\  \frac{1}{(2\pi)^{Nd/2}|\hat{\pmb{\Sigma}}_{MLE}|^{N/2}}(\prod_{i=1}^{d}\lambda_i)^{N/2}exp\left\{-\frac{N}{2}\sum_{i=1}^{d}\lambda_i\right\}\\
&=\underset{\lambda_1...\lambda_d}{argmax}\ (\prod_{i=1}^{d}\lambda_i)^{N/2}exp\left\{-\frac{N}{2}\sum_{i=1}^{d}\lambda_i\right\}\\
&=\underset{\lambda_1...\lambda_d}{argmin}\ -log(\prod_{i=1}^{d}\lambda_i)+\sum_{i=1}^{d}\lambda_i\\
&=\underset{\lambda_1...\lambda_d}{argmin}\ -\sum_{i=1}^{d}log(\lambda_i)+\sum_{i=1}^{d}\lambda_i\\
&=\underset{\lambda_1...\lambda_d}{argmin}\ \sum_{i=1}^{d}\lambda_i-log(\lambda_i)\\
\end{split}
\end{equation}

Take derivative, setting to zero:
$$\nabla\pmb{\lambda}= \begin{bmatrix}
1-\frac{1}{\lambda_1}\\
1-\frac{1}{\lambda_2}\\
\vdots\\
1-\frac{1}{\lambda_d}\\
\end{bmatrix} = \pmb{0}$$

Obviously $\lambda_i=1$ for all $i$.
\subsection*{(b)}
By maximizing the likelihood function:
$$\hat{\pmb{\Sigma}} = \underset{\pmb{\Sigma}}{argmax}\ p(\pmb{x}_1,...,\pmb{x}_N|\pmb{\Sigma})$$
\begin{equation}
\begin{split}
\hat{\pmb{\Sigma}}&=\underset{\pmb{\Sigma}}{argmax}\ \frac{1}{(2\pi)^{Nd/2}|\pmb{\Sigma}|^{N/2}}exp\left\{-\frac{1}{2}\sum_{i=1}^{N}(\pmb{x}_i-\pmb{\mu})^T\pmb{\Sigma}^{-1}(\pmb{x}_i-\pmb{\mu})\right\}\\
&=\underset{\pmb{\Sigma}}{argmax}\  \frac{|\pmb{\Sigma}^{-1}|^{N/2}}{(2\pi)^{Nd/2}}exp\left\{-\frac{1}{2}tr\left[\pmb{\Sigma}^{-1}\sum_{i=1}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]\right\}\\
&=\underset{\pmb{\Sigma}}{argmin}\ -\frac{N}{2}log(|\pmb{\Sigma}^{-1}|)+\frac{Nd}{2}log(2\pi)+\frac{1}{2}tr\left[\pmb{\Sigma}^{-1}\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]\\
&=\underset{\pmb{\Sigma}}{argmin}\ -Nlog(|\pmb{\Sigma}^{-1}|)+tr\left[\pmb{\Sigma}^{-1}\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]
\end{split}
\end{equation}
\pagebreak

\noindent Set $\pmb{P} = \pmb{\Sigma}^{-1}$\\
\begin{equation}
\begin{split}
\hat{\pmb{P}} = \underset{\pmb{P}}{argmin}\ -Nlog(|\pmb{P}|)+tr\left[\pmb{P}\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\right]
\end{split}
\end{equation}
Take derivative, setting to zero:
\begin{equation}
\begin{split}
-\frac{N}{|\pmb{P}|}|\pmb{P}|\pmb{P}^{-1}&+\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T = 0\\
N\pmb{P}^{-1}&=\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\\
\pmb{P}^{-1}&=\frac{1}{N}\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T\\
\end{split}
\end{equation}
thus,

$$\hat{\pmb{\Sigma}}_{MLE}=\frac{1}{N}\sum_{i=0}^{N}(\pmb{x}_i-\pmb{\mu})(\pmb{x}_i-\pmb{\mu})^T$$

\section*{Exercise 2}

\pagebreak
\section*{Exercise 3}
\end{document}